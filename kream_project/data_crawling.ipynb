{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyautogui as p\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chrome_driver():\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 솔드아웃 상품 목록, 상세 정보 Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "엑셀 파일로 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 솔드아웃 상품 목록, 상세 정보 crawling\n",
    "# 상위 몇개까지만 할지는 미정\n",
    "# browser.back() 적용 아직 안함\n",
    "browser = set_chrome_driver()\n",
    "result = []\n",
    "\n",
    "base_url = \"https://www.soldout.co.kr\"\n",
    "url_soldout = base_url + \"/search/product/list\"\n",
    "browser.get(url=url_soldout)\n",
    "\n",
    "# 스크롤 내리기\n",
    "interval = 3\n",
    "\n",
    "prev_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(interval)\n",
    "    cur_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    if cur_height == prev_height:\n",
    "        break\n",
    "\n",
    "    prev_height = cur_height\n",
    "  \n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "products = soup.find_all(\"div\", class_=\"product-item\")\n",
    "\n",
    "\n",
    "products = products[:1000]\n",
    "# 상품 목록 페이지에서 데이터 crawling\n",
    "for info in products:\n",
    "    # 상품 한글명\n",
    "    name_kor = info.find(\"p\", class_=\"product-name\").text\n",
    "    # 브랜드명\n",
    "    brand = info.find(\"span\", class_=\"brand-logo__text\").text\n",
    "    # 이미지 주소\n",
    "    img_tag = info.find(\"img\")\n",
    "    img_url = img_tag[\"src\"]\n",
    "    # 상세페이지 주소\n",
    "    product_detail_tag = info.find(\"a\",class_=\"link-for-seo\")\n",
    "    # https://www.soldout.co.kr/trade/detail/5534466\n",
    "    product_detail_url = base_url + product_detail_tag[\"href\"]\n",
    "\n",
    "    # 상품 상세 페이지에서 데이터 crawling\n",
    "    product_detail_url_list = []\n",
    "    product_detail_url_list.append(product_detail_url)\n",
    "\n",
    "    for url in product_detail_url_list:\n",
    "        browser.get(url=url)\n",
    "\n",
    "        # 스크롤 내리기\n",
    "        interval = 3\n",
    "        prev_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(interval)\n",
    "            cur_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if cur_height == prev_height:\n",
    "                break\n",
    "\n",
    "            prev_height = cur_height\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "       \n",
    "        # 상품 영어명\n",
    "        name_eng = soup.select_one(\"#__layout > div > div.layout-container > div > div.item-container__in > div.container-right > div.item_info__wrap > p\")\n",
    "        # 모델번호\n",
    "        model_no = soup.select_one(\"#__layout > div > div.layout-container > div > div.item-container__in > div.container-right > div:nth-child(8) > dl:nth-child(3) > dd\")\n",
    "        # 출시일(DataFrame에서 날짜형태로 바꾸기)\n",
    "        release_date = soup.select_one(\"#__layout > div > div.layout-container > div > div.item-container__in > div.container-right > div:nth-child(8) > dl:nth-child(4) > dd\")\n",
    "        # 색상\n",
    "        color = soup.select_one(\"#__layout > div > div.layout-container > div > div.item-container__in > div.container-right > div:nth-child(8) > dl.product-info__dl.color > dd\")\n",
    "        # 원래 가격(\"원\" 제거 후 DataFrame에서 int형으로 바꾸기)\n",
    "        original_price = soup.select_one(\"#__layout > div > div.layout-container > div > div.item-container__in > div.container-right > div:nth-child(8) > dl:nth-child(6) > dd\")\n",
    "\n",
    "        if name_eng and model_no and release_date and color and original_price:\n",
    "            name_eng = name_eng.text\n",
    "            model_no = model_no.text\n",
    "            release_date = release_date.text\n",
    "            color = color.text\n",
    "            original_price = original_price.text\n",
    "        else:\n",
    "            name_eng = np.nan\n",
    "            model_no = np.nan\n",
    "            release_date = np.nan\n",
    "            color = np.nan\n",
    "            original_price = np.nan\n",
    "\n",
    "        # print(brand, name_kor, name_eng, model_no, release_date, color, original_price, img_url, product_detail_url)\n",
    "\n",
    "    result.append([brand, name_kor, name_eng, model_no, release_date, color, original_price, img_url, product_detail_url])\n",
    "\n",
    "columns = [\"Brand\", \"Name_Kor\", \"Name_Eng\", \"Model_No\", \"Release_Date\", \"Color\", \"Original_Price\", \"Image_URL\", \"Product_Detail_URL\"]\n",
    "df = pd.DataFrame(result, columns=columns)\n",
    "df.to_excel(\"./data/soldout_products2.xlsx\", index=False)\n",
    "\n",
    "wb = load_workbook(\"./data/soldout_products2.xlsx\")\n",
    "ws = wb.active\n",
    "\n",
    "# 열의 너비 설정\n",
    "ws.column_dimensions[\"A\"].width = 20\n",
    "ws.column_dimensions[\"B\"].width = 100\n",
    "ws.column_dimensions[\"C\"].width = 100\n",
    "ws.column_dimensions[\"D\"].width = 50\n",
    "ws.column_dimensions[\"E\"].width = 50\n",
    "ws.column_dimensions[\"F\"].width = 50\n",
    "ws.column_dimensions[\"G\"].width = 50\n",
    "ws.column_dimensions[\"H\"].width = 100\n",
    "ws.column_dimensions[\"I\"].width = 60\n",
    "\n",
    "# 최종 엑셀 파일로 저장\n",
    "wb.save(\"./data/soldout_products2.xlsx\")\n",
    "\n",
    "print(len(products))\n",
    "print(\"엑셀 파일로 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 솔드아웃 거래내역 Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".btn-show-all\"}\n  (Session info: chrome=126.0.6478.127); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00D6C1C3+27395]\n\t(No symbol) [0x00D03DC4]\n\t(No symbol) [0x00C01B7F]\n\t(No symbol) [0x00C42C65]\n\t(No symbol) [0x00C42D3B]\n\t(No symbol) [0x00C7EC82]\n\t(No symbol) [0x00C639E4]\n\t(No symbol) [0x00C7CB24]\n\t(No symbol) [0x00C63736]\n\t(No symbol) [0x00C37541]\n\t(No symbol) [0x00C380BD]\n\tGetHandleVerifier [0x01023A93+2876371]\n\tGetHandleVerifier [0x01077F5D+3221661]\n\tGetHandleVerifier [0x00DED634+556916]\n\tGetHandleVerifier [0x00DF474C+585868]\n\t(No symbol) [0x00D0CE04]\n\t(No symbol) [0x00D09818]\n\t(No symbol) [0x00D099B7]\n\t(No symbol) [0x00CFBF0E]\n\tBaseThreadInitThunk [0x75D7FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x774180CE+286]\n\tRtlGetAppContainerNamedObjectPath [0x7741809E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 거래내역 전체보기 버튼 클릭\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m view_all_btn \u001b[38;5;241m=\u001b[39m \u001b[43mbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbtn-show-all\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m view_all_btn\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     45\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\source\\pythonsource\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\source\\pythonsource\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\source\\pythonsource\\venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".btn-show-all\"}\n  (Session info: chrome=126.0.6478.127); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00D6C1C3+27395]\n\t(No symbol) [0x00D03DC4]\n\t(No symbol) [0x00C01B7F]\n\t(No symbol) [0x00C42C65]\n\t(No symbol) [0x00C42D3B]\n\t(No symbol) [0x00C7EC82]\n\t(No symbol) [0x00C639E4]\n\t(No symbol) [0x00C7CB24]\n\t(No symbol) [0x00C63736]\n\t(No symbol) [0x00C37541]\n\t(No symbol) [0x00C380BD]\n\tGetHandleVerifier [0x01023A93+2876371]\n\tGetHandleVerifier [0x01077F5D+3221661]\n\tGetHandleVerifier [0x00DED634+556916]\n\tGetHandleVerifier [0x00DF474C+585868]\n\t(No symbol) [0x00D0CE04]\n\t(No symbol) [0x00D09818]\n\t(No symbol) [0x00D099B7]\n\t(No symbol) [0x00CFBF0E]\n\tBaseThreadInitThunk [0x75D7FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x774180CE+286]\n\tRtlGetAppContainerNamedObjectPath [0x7741809E+238]\n"
     ]
    }
   ],
   "source": [
    "# 솔드아웃 로그인\n",
    "browser = set_chrome_driver()\n",
    "browser.get(url_soldout)\n",
    "result2 = []\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "login_button = browser.find_element(By.XPATH, '//*[@id=\"__layout\"]/div/div[1]/header/div/ul/li[1]/a')\n",
    "login_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "id_input = browser.find_element(By.CSS_SELECTOR, \"#__layout > div > div.layout-container > div > form > div:nth-child(1) > div > input\")\n",
    "id_input.send_keys(\"uj05273\")\n",
    "\n",
    "pwd_input = browser.find_element(By.CSS_SELECTOR, \"#__layout > div > div.layout-container > div > form > div:nth-child(2) > div > input\")\n",
    "pwd_input.send_keys(\"brian981103\")\n",
    "\n",
    "signin_button = browser.find_element(By.CLASS_NAME, \"btn-primary\")\n",
    "signin_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# 거래내역 crawling\n",
    "for info in products[:100]:\n",
    "    # 상세페이지 주소\n",
    "    product_detail_tag = info.find(\"a\",class_=\"link-for-seo\")\n",
    "    # https://www.soldout.co.kr/trade/detail/5534466\n",
    "    product_detail_url = base_url + product_detail_tag[\"href\"]\n",
    "\n",
    "    # 상품 상세 페이지에서 데이터 crawling\n",
    "    product_detail_url_list = []\n",
    "    product_detail_url_list.append(product_detail_url)\n",
    "\n",
    "    for url in product_detail_url_list:\n",
    "        #print(url)\n",
    "        browser.get(url=url)\n",
    "\n",
    "        interval = 3\n",
    "\n",
    "        prev_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(interval)\n",
    "            cur_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if cur_height == prev_height:\n",
    "                break\n",
    "\n",
    "            prev_height = cur_height\n",
    "        \n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        # 거래내역 전체보기 버튼 클릭\n",
    "        view_all_btn = browser.find_element(By.CLASS_NAME, \"btn-show-all\")\n",
    "        view_all_btn.click()\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        # 상품 목록 데이터 중 한글이름과 merge 하기 위함\n",
    "        name_kor = soup.find(\"p\",class_=\"name_kor\")\n",
    "        if name_kor:\n",
    "            name_kor = name_kor.text\n",
    "        \n",
    "        # 스크롤을 위해 마우스 중앙으로 옮기기\n",
    "        p.moveTo(1270,815,0.5)\n",
    "        p.click()\n",
    "\n",
    "        modal_content = browser.find_element(By.CSS_SELECTOR, \"body > div.trade_modal.BaseModal > div > div > div.base-table.trade_modal__table.modal-table > table > tbody\")\n",
    "        prev_height = browser.execute_script(\"return arguments[0].scrollHeight\", modal_content)\n",
    "\n",
    "        # 모달 창 스크롤 내리기\n",
    "        while True:\n",
    "            browser.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", modal_content)\n",
    "            time.sleep(1)\n",
    "            cur_height = browser.execute_script(\"return arguments[0].scrollHeight\", modal_content)\n",
    "\n",
    "            if cur_height == prev_height:\n",
    "                break\n",
    "\n",
    "            prev_height = cur_height\n",
    "        \n",
    "        soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "        trades = soup.select(\"body > div.trade_modal.BaseModal > div > div > div.base-table.trade_modal__table.modal-table > table > tbody > tr\")\n",
    "        \n",
    "        for trade in trades:\n",
    "            trade_dates = trade.select_one(\"tbody > tr > td:nth-child(1)\")\n",
    "            trade_sizes = trade.select_one(\"tbody > tr > td:nth-child(2)\")\n",
    "            trade_prices = trade.select_one(\"tbody > tr > td:nth-child(3) > span\")\n",
    "\n",
    "            if trade_dates and trade_sizes and trade_prices:\n",
    "                trade_dates = trade_dates.text\n",
    "                trade_sizes = trade_sizes.text\n",
    "                trade_prices = trade_prices.text\n",
    "        \n",
    "            result2.append([name_kor,trade_dates,trade_sizes,trade_prices])\n",
    "\n",
    "\n",
    "columns = [\"Name_Kor\", \"Trade_Dates\", \"Trade_Sizes\", \"Trade_Prices\"]\n",
    "df = pd.DataFrame(result2, columns=columns)\n",
    "df.to_excel(\"./data/soldout_trades.xlsx\", index=False)\n",
    "\n",
    "wb = load_workbook(\"./data/soldout_trades.xlsx\")\n",
    "ws = wb.active\n",
    "\n",
    "# 열의 너비 설정\n",
    "ws.column_dimensions[\"A\"].width = 50\n",
    "ws.column_dimensions[\"B\"].width = 35\n",
    "ws.column_dimensions[\"C\"].width = 30\n",
    "ws.column_dimensions[\"D\"].width = 30\n",
    "\n",
    "wb.save(\"./data/soldout_trades.xlsx\")\n",
    "\n",
    "print(\"엑셀 파일로 저장 완료!\")\n",
    "            \n",
    "    # X버튼으로 나가기 -> 뒤로가기 버튼\n",
    "    # x_btn = browser.find_element(By.XPATH,'/html/body/div[5]/div/header/div[3]/button')\n",
    "    # x_btn.click()\n",
    "    # browser.back()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kream에서 상품 상세페이지, 상품코드 crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 휴일은 딸과 함께~ \n"
     ]
    }
   ],
   "source": [
    "browser = set_chrome_driver()\n",
    "base_url = \"https://kream.co.kr\"\n",
    "url = base_url + \"/search\"\n",
    "browser.get(url=url)\n",
    "\n",
    "# 스크롤 내리기\n",
    "# interval = 3\n",
    "# prev_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# while True:\n",
    "#     browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "#     time.sleep(interval)\n",
    "#     cur_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "#     if cur_height == prev_height:\n",
    "#         break\n",
    "\n",
    "#     prev_height = cur_height\n",
    "\n",
    "# time.sleep(4)\n",
    "\n",
    "# 상품마다 각 상세 페이지 가져오기\n",
    "soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "products = soup.find_all(\"div\",class_=\"search_result_item product\")\n",
    "for product in products[:1]:\n",
    "    product_detail_url = product.find(\"a\",class_=\"item_inner\")[\"href\"]\n",
    "    product_detail_urls = base_url + product_detail_url\n",
    "    \n",
    "    # 상품 상세 페이지에서 데이터 crawling\n",
    "    product_detail_url_list = []\n",
    "    product_detail_url_list.append(product_detail_urls)\n",
    "\n",
    "    for url in product_detail_url_list:\n",
    "        browser.get(url=url)\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "        model_no = soup.select_one(\"div.product_info_wrap > div > dl > div:nth-child(3) > div.product_info\")\n",
    "        if model_no:\n",
    "            model_no = model_no.text\n",
    "            # print(model_no)\n",
    "        \n",
    "        # https://kream.co.kr/social/products/12831\n",
    "        social_url = base_url + \"/social\" + product_detail_url\n",
    "        \n",
    "        browser.get(url=social_url)\n",
    "        soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "        # 리뷰 전체를 보기위하여 스크롤 내리기\n",
    "        interval = 3\n",
    "        prev_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            time.sleep(interval)\n",
    "            cur_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if cur_height == prev_height:\n",
    "                break\n",
    "\n",
    "            prev_height = cur_height\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "        socials = soup.find_all(\"div\",class_=\"feed_card\")\n",
    "        social_text_list = []\n",
    "        for social in socials:\n",
    "            social_text = social.find(\"p\",class_=\"text_box\").text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://kream.co.kr/products/21935\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 상세페이지 주소\u001b[39;00m\n\u001b[0;32m     50\u001b[0m product_detail_tag \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m product_detail_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://kream.co.kr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mproduct_detail_tag\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 상품 상세 페이지에서 데이터 crawling\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# https://kream.co.kr/products/21935\u001b[39;00m\n\u001b[0;32m     56\u001b[0m product_detail_url_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "browser = set_chrome_driver()\n",
    "result = []\n",
    "\n",
    "# base_url = \"https://www.soldout.co.kr\"\n",
    "# url_soldout = base_url + \"/search/product/list\"\n",
    "browser.get(url=\"https://kream.co.kr/search\")\n",
    "\n",
    "# 스크롤 내리기\n",
    "interval = 3\n",
    "total_items = 0\n",
    "target_items = 3\n",
    "prev_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while total_items < target_items:\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(interval)\n",
    "    cur_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "    products = soup.find_all(\"div\", class_=\"product\")\n",
    "\n",
    "    if cur_height == prev_height:\n",
    "        break\n",
    "\n",
    "    prev_height = cur_height\n",
    "\n",
    "    total_items += len(products)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "# products = soup.find_all(\"div\", class_=\"product\")\n",
    "products = products[:target_items]\n",
    "# 상품 목록 페이지에서 데이터 crawling\n",
    "\n",
    "for info in products:\n",
    "    # 상품 한글명\n",
    "    name_kor = info.find(\"p\", class_=\"translated_name\")\n",
    "    # 상품 영어명\n",
    "    name_eng = info.find(\"p\", class_=\"name\")\n",
    "    # 브랜드명\n",
    "    brand = info.find(\"p\", class_=\"product_info_brand\")\n",
    "    \n",
    "    # 이미지 주소\n",
    "    img_tag = info.find(\"img\")\n",
    "    img_url = img_tag[\"src\"]\n",
    "    # 상세페이지 주소\n",
    "    product_detail_tag = info.find(\"a\",class_=\"item_inner\")\n",
    "    \n",
    "    product_detail_url = \"https://kream.co.kr\" + product_detail_tag[\"href\"]\n",
    "\n",
    "    # 상품 상세 페이지에서 데이터 crawling\n",
    "    # https://kream.co.kr/products/21935\n",
    "    product_detail_url_list = []\n",
    "    product_detail_url_list.append(product_detail_url)\n",
    "\n",
    "    if name_kor and name_eng and brand:\n",
    "        name_kor = name_kor.text\n",
    "        name_eng = name_eng.text\n",
    "        brand = brand.text\n",
    "\n",
    "    for url in product_detail_url_list:\n",
    "        # browser.get(url=url)\n",
    "        \n",
    "\n",
    "#         # 스크롤 내리기\n",
    "#         interval = 3\n",
    "#         prev_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "#         while True:\n",
    "#             browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "#             time.sleep(interval)\n",
    "#             cur_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "#             if cur_height == prev_height:\n",
    "#                 break\n",
    "\n",
    "#             prev_height = cur_height\n",
    "\n",
    "#         time.sleep(2)\n",
    "\n",
    "#         soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "       \n",
    "       \n",
    "#         # 모델번호\n",
    "#         model_no = soup.select_one(\"div.column_top > div.product_info_wrap > div > dl > div:nth-child(3) > div.product_info\")\n",
    "#         # 출시일(DataFrame에서 날짜형태로 바꾸기)\n",
    "#         release_date = soup.select_one(\"div.column_top > div.product_info_wrap > div > dl > div:nth-child(4) > div.product_info\")\n",
    "#         # 색상\n",
    "#         color = soup.find(\"div\",class_=\"color-target\")\n",
    "#         # 원래 가격(\"원\" 제거 후 DataFrame에서 int형으로 바꾸기)\n",
    "#         original_price = soup.select_one(\"div.column_top > div.product_info_wrap > div > dl > div:nth-child(2) > div.product_info\")\n",
    "\n",
    "#         if name_eng and model_no and release_date and color and original_price:\n",
    "#             model_no = model_no.text\n",
    "#             release_date = release_date.text\n",
    "#             color = color.text\n",
    "#             original_price = original_price.text\n",
    "#         else:\n",
    "#             name_eng = np.nan\n",
    "#             model_no = np.nan\n",
    "#             release_date = np.nan\n",
    "#             color = np.nan\n",
    "#             original_price = np.nan\n",
    "\n",
    "#         # print(brand, name_kor, name_eng, model_no, release_date, color, original_price, img_url, product_detail_url)\n",
    "\n",
    "#     result.append([brand, name_kor, name_eng, model_no, release_date, color, original_price, img_url, product_detail_url])\n",
    "# print(brand, name_kor, name_eng, model_no, release_date, color, original_price, img_url, product_detail_url)\n",
    "# columns = [\"Brand\", \"Name_Kor\", \"Name_Eng\", \"Model_No\", \"Release_Date\", \"Color\", \"Original_Price\", \"Image_URL\", \"Product_Detail_URL\"]\n",
    "# df = pd.DataFrame(result, columns=columns)\n",
    "# df.to_excel(\"./data/soldout_products.xlsx\", index=False)\n",
    "\n",
    "# wb = load_workbook(\"./data/soldout_products.xlsx\")\n",
    "# ws = wb.active\n",
    "\n",
    "# # 열의 너비 설정\n",
    "# ws.column_dimensions[\"A\"].width = 20\n",
    "# ws.column_dimensions[\"B\"].width = 100\n",
    "# ws.column_dimensions[\"C\"].width = 100\n",
    "# ws.column_dimensions[\"D\"].width = 50\n",
    "# ws.column_dimensions[\"E\"].width = 50\n",
    "# ws.column_dimensions[\"F\"].width = 50\n",
    "# ws.column_dimensions[\"G\"].width = 50\n",
    "# ws.column_dimensions[\"H\"].width = 100\n",
    "# ws.column_dimensions[\"I\"].width = 60\n",
    "\n",
    "# # 최종 엑셀 파일로 저장\n",
    "# wb.save(\"./data/soldout_products.xlsx\")\n",
    "\n",
    "# print(len(products))\n",
    "# print(\"엑셀 파일로 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
